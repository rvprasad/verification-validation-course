# Readings in Verificaiton and Validation Course

These are the contents of Verification and Validation course offered at Kansas State University in Fall'15-'17. This is an upper-level graduate course. It requires students to have taken graduate-level courses in software engineering and software specification and to have some experience developing software.


## Format

 At the beginning of the semester, a subset of papers from the following list is selected for discussion. We consdier the interests and strengths of students enrolled in the course while selecting the papers. For example, if most of the students are employed in the industry, then we pick more industry paper (i.e., tools, tech transfer, studies). Then, each student is assigned a set of papers from the set.

 In each class, one student presents an assigned paper to the rest of the class while the rest of the class participates in discussing the assigned paper. The presenter is required to be aware of the entire content of the paper and to try to answer any questions raised during the discussion. At times, the presenter may not understand some parts of the paper. In such situation, he/she can consult the instructor about such parts of the paper before presenting the paper or he/she can state his/her understanding and seek clarification from the instructor in class while presenting the paper. The instructor will steer the in-class discussion (e.g., by asking questions/clarifications about specific parts of papers) and clarify any doubts about the paper that were unanswered by the presenter.

 When there are more than few students in a class, students are required to summarize the paper before the class, update the summary after discussing the paper, and then submit the updated summary to the instructor for evaluation.


## Discussed Papers

1. [A Decade of Software Model Checking with SLAM.](papers/CACM11-Ball.pdf) Thomas Ball, Vladimir Levin, and Sriram Rajamani. CACM'11. **Model Checking**
1. [A Randomized Scheduler with Probabilistic Guarantees of Finding Bugs.](papers/ASPLOS10-Burckhardt.pdf) Sebastian Burckhardt, Pravesh Kothari, Madanlal Musuvathi, and Santosh Nagarakatte. ASPLOS'10. **Bug Finding**
2. [A Survey of Symbolic Execution Techniques.] Roberto Baldoni, Emilio Coppa, Daniele Cono D'Elia, Camil Demetrescu, and Irene Finocchi. Arxiv'16. **Symbolic Execution**
1. [Achievements, open problems and challenges for search based software testing.](papers/ICST15-Harman.pdf) Mark Harman, Yue Jia and Yuanyuan Zhang. ICST'15. **Testing**
1. [Addressing Dynamic Issues of Program Model Checking.](papers/SPIN01-Lerda.pdf) Flavio Lerda, and Willem Visser. SPIN'01. **Model Checking**
1. [An Analysis and Survey of the Development of Mutation Testing.](papers/TSE11-Jia.pdf) Yue Jia and Mark Harman. TSE'11. **Testing**
1. [An introduction to Property-based Testing.](papers/http://fsharpforfunandprofit.com/posts/property-based-testing/) **Testing**
1. [Astree: From Research to Industry.](papers/SAS07-Delmas.pdf) David Delmas and Jean Souyris. SAS'07. **Static Analysis**
1. [Automated White-box Testing.](papers/NDSS08-Godefroid.pdf) Patrice Godefroid, Michael Levin, and David Molnar. NDSS'08. **Fuzzing, Testing**
1. [Bandera: a source-level interface for model checking Java programs.](papers/ICSE00-Corbett.pdf) James C. Corbett, Matthew B. Dwyer, John Hatcliff, and Robby. ICSE'00. **Model Checking, Tooling**
1. [Basic Concepts and Taxonomy of Dependable and Secure Computing.](papers/TDSC04-Avizienis.pdf) Algridas Avizienis and Brian Randell. IEEE Transactions on Dependable and Secure Computing 2004. **Concepts**
1. [Boeing Flies on 99% Ada.](papers/http://archive.adaic.com/projects/atwork/boeing.html)
1. [CUTE: a concolic unit testing engine for C.](papers/FSE05-Sen.pdf) Koushik Sen, Darko Marinov, and Gul Agha. FSE'05. **Symbolic Execution, Testing**
1. [Cadena: An Integrated Development, Analysis, and Verification Environment for Component-based Systems.](papers/ICSE03-Hatcliff.pdf) John Hatcliff, William Deng, Matthew B. Dwyer, Georg Jung, and Venkatesh Prasad Ranganath. ICSE'03. **MDD, Tooling**
1. [Choosing Properties for Property-based Testing.] (http://fsharpforfunandprofit.com/posts/property-based-testing-2/) **Testing**
1. [Compatibility Testing via Patterns-based Trace Comparison.](papers/ASE14-Ranganath.pdf) Venkatesh-Prasad Ranganath, Pradip Vallathol, and Pankaj Gupta. ASE'14. **Tech Transfer, Tooling**
1. [DART: Directed Automated Random Testing.](papers/PLDI05-Godefroid.pdf) Patrice Godefroid, Nils Klarlund, and Koushik Sen. PLDI'05. **Symbolic Execution, Testing**
1. [DARWIN: An Approach for Debugging Evolving Programs.](papers/TOSEM12-Qi.pdf) Dawei Qi, Abhik Roychoudoury, Zhenkai Liang, and Kapil Vaswani. TOSEM'12. **Testing**
1. [Differential Testing for Software.](papers/DTJ98-McKeeman.pdf) William M. McKeeman. DTJ'98. **Testing**
1. [Enabling Efficient Partial Order Reductions for Model Checking Object-Oriented Programs Using Static Calculation of Program Dependences.](papers/Santos-TR2007-Ranganath.pdf) Venkatesh Prasad Ranganath, John Hatcliff, and Robby. Santos-TR2007-2. **Model Checking, POR**
1. [Evaluating Static Analysis Defect Warnings On Production Software.](papers/PAST07-Ayewah.pdf) Nathaniel Ayewah, William Pugh, J. David Morgenthaler, John Penix, and YuQian Zhou. PASTE'07. **Evaluation, Static Analysis, Tooling**
1. [Evaluating the Effectiveness of Slicing for Model Reduction of Concurrent Object-Oriented Programs.](papers/TACAS06-Dwyer.pdf) Matthew B. Dwyer, John Hatcliff, Matthew Hoosier, Venkatesh Prasad Ranganath, Robby, and Todd Wallentine. TACAS'06. **Evaluation, Program Slicing, Model Checking**
1. [Feedback-directed random test generation.](papers/ICSE07-Pacheco.pdf) Carlos Pacheco, Shuvendu K. Lahiri, Michael D. Ernst, and Thomas Ball. ICSE'07. **Testing**
1. [How AWS uses formal methods.](papers/CACM15-Newcombe.pdf) Newcombe, Rath, Zhang, Munteanu, Brooker, and Dearduff. CACM'15e. **Specification**
1. [How Complex Systems Fail.](papers/HowComplexSystemsFai.pdf) Richard I. Cook. **Systems and Failures**
1. [How Microsoft built, and is still building, Windows 10.](papers/http://venturebeat.com/2015/08/13/how-microsoft-built-and-is-still-building-windows-10/) **Testing**
1. [Isolating Cause-Effect Chains from Computer Programs.](papers/FSE02-Zeller.pdf) Andreas Zeller. FSE'02. **Debugging**
1. [Locating Causes of Program Failures.](papers/ICSE05-Cleve.pdf) Holger Cleve and Andreas Zeller. ICSE'05. **Debugging**
1. [Model-Based Quality Assurance of Protocol Documentation: Tools and Methodology.](papers/ICST08-Grieskamp.pdf) Wolfgang Grieskamp, Nicolas Kicillof, Keith Stobie, and Victor Braberman. STVR (ICST'08). **Specification**
1. [Model-Based Testing: Where does it stand?](papers/CACM15-Binder.pdf) Robert V. Binder, Bruno Legeard, and Anne Kramer. CACM'15. **Evaluation, Testing**
1. [Model Checking Programs.](papers/Visser-ASE03.pdf) Willem Visser, Klaus Havelund, Guillaume Brat, SeungJoon Park, and Flavio Lerda. ASE'03. **Model Checking**
1. [Moving Fast with Software Verication.](papers/NFM15-Calcagno.pdf) Cristiano Calcagno, Dino Distefano, Jeremy Dubreil, Dominik Gabi, Pieter Hooimeijer, Martino Luca, Peter O'Hearn, Irene Papakonstantinou, Jim Purbrick, and Dulma Rodriguez. NFM'15. **Static Analysis, Tech Transfer**
1. [Pex - White Box Test Generation for .NET.](papers/TAP08-Tillmann.pdf) Nikolai Tillmann and Peli de Halleux. TAP'08. **Symbolic Execution, Testing**
1. [Race Conditions, Distribution, Interactions -- Testing the Hard Stuff and Staying Sane.](papers/https://vimeo.com/68383317) **Testing**
1. [Realizing quality improvement through test driven development: results and experiences of four industrial teams.](papers/ESE08-Nagappan.pdf) Nachiappan Nagappan, E. Michael Maximillien, Thirumalesh Bhat, and Laurie Williams. ESE'08 **TDD**
1. [Regression Testing Minimisation, Selection and Prioritisation : A Survey.](papers/STVR12-Yoo.pdf) Yoo and Harman. STVR'12. **Concepts, Testing**
1. [SLAM and Static Driver Verifier: Technology Transfer for Formal Methods inside Microsoft.](papers/IFM04-Ball.pdf) Thomas Ball, Byron Cook, Vladimir Levin, and Sriram Rajamani. IFM'04. **Tech Transfer**
1. [Satisfiability Modulo Theories: Introduction and Applications.](papers/CACM11-Moura.pdf) Leonardo De Moura and Nikolaj Bjorner. CACM'11. **SMT**
1. [Simple Testing Can Prevent Most Critical Failures: An Analysis of Production Failures in Distributed Data-Intensive Systems.](papers/OSDI14-Yuan.pdf) Ding Yuan, Yu Luo, Xin Zhuang, Guilherme Renna Rodrigues, Xu Zhao, Yongle Zhang, Pranay U. Jain, and Michael Stumm. OSDI'14. **Testing**
1. [Software Model Checking.](papers/ACMCS09-Jhala.pdf) Ranjit Jhala and Rupak Majumdar. ACM CS'09. **Model Checking**
1. [Symbolic Execution for Software Testing: Three Decades Later.](papers/CACM13-Cadar.pdf) Cristian Cadar and Koushik Sen. CACM'13. **Symbolic Execution, Testing**
1. [The Model Checker SPIN.](papers/TSE97-Holzmann.pdf) Gerard J. Holzmann. TSE'97 **Model Checking**
1. [The Science of Brute Force.](papers/CACM17-Heule.pdf) Marijn J. H. Heule and Oliver Kullmann. CACM'17. **SMT**
1. [Transferring an Automated Test Generation Tool to Practice: From Pex to Fakes and Code Digger.](papers/ASE14-Tillmann.pdf) Nikolai Tillmann, Peli De Halleux, and Tao Xie. ASE'14. **Tech Transfer**
1. [Who Builds a House without Drawing Blueprints?](papers/CACM15-Lamport.pdf) Leslie Lamport. CACM'15. **Specification**
1. [Why Amazon chose TLA+?](papers/ABZ14-Newcombe.pdf) Chris Newcombe. ABZ'14. **Specification**


## Attribution

Copyright (c) 2015, Venkatesh-Prasad Ranganath

<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.

**Compiler: Venkatesh-Prasad Ranganath**
